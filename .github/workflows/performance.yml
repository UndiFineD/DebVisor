name: Performance Benchmarks

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main ]


defaults:
  run:
    shell: bash

permissions:
  contents: read
  issues: write

jobs:
  benchmark:
    runs-on: self-hosted
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt pytest pytest-benchmark

      - name: Run benchmarks
        run: |
          pytest --benchmark-json=benchmark-results.json || true

      - name: Check benchmark results presence
        id: check-bench
        run: |
          if [ -f benchmark-results.json ] && [ -s benchmark-results.json ]; then
            python -c "import json; json.load(open('benchmark-results.json'))" >/dev/null 2>&1 && echo "has_results=true" >> $GITHUB_OUTPUT || echo "has_results=false" >> $GITHUB_OUTPUT
          else
            echo "has_results=false" >> $GITHUB_OUTPUT
          fi

      - name: Store benchmark result
        if: github.ref == 'refs/heads/main' && steps.check-bench.outputs.has_results == 'true'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false

      - name: Compare with baseline
        if: github.event_name == 'pull_request' && steps.check-bench.outputs.has_results == 'true'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false

      - name: Detect regression and summarize
        id: perf_summary
        if: steps.check-bench.outputs.has_results == 'true'
        run: |
          if grep -qi 'ALERT' benchmark-results.json 2>/dev/null; then
            echo "regression=true" >> $GITHUB_OUTPUT
          else
            echo "regression=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload benchmark results artifact
        if: steps.check-bench.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
          retention-days: 90
    outputs:
      regression: ${{ steps.perf_summary.outputs.regression }}

  notify:
    needs: [ benchmark ]
    if: needs.benchmark.outputs.regression == 'true'
    uses: ./.github/workflows/_notify.yml
    with:
      workflow_name: 'Performance Benchmarks'
      status: 'failure'
      issue_label: 'performance'
      details: |
        Regression markers detected in benchmark output.
        Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        Branch: ${{ github.ref }}
        See benchmark artifacts for details.
    secrets:
      token: ${{ secrets.GITHUB_TOKEN }}
