# DebVisor Monitoring - Complete Reference\n\nThis directory contains DebVisor's monitoring, observability, and synthetic testing infrastructure.\n\n## Directory Structure\n\n    monitoring/\n    +-- README.md                          # This file\n    +-- FIXTURES_GUIDE.md                 # Synthetic metrics fixtures guide\n    +-- fixtures/                         # Optional synthetic metrics (labs only)\n    |   +-- README.md\n    |   +-- FIXTURES_GUIDE.md\n    |   +-- edge-lab.yaml                # Lab environment fixture\n    |   +-- edge-lab-deployment.yaml     # Lab environment generator\n    |   +-- generator/                   # Synthetic metrics generator source\n    |   +-- kustomize/                   # Environment-specific customization\n    +-- grafana/                         # Grafana dashboards and provisioning\n        +-- README.md\n        +-- dashboards/                  # Dashboard JSON files\n        +-- provisioning/\n        |   +-- dashboards/              # Dashboard provisioning config\n        |   +-- alerting/                # Alert rules (provisional)\n        +-- manifests/                   # Kubernetes manifests (optional)\n\n## Core Components\n\n### 1. Prometheus\n\n- *Purpose:**Time-series metrics collection, storage, and querying\n\n### DebVisor Integration\n\n- Scrapes metrics from all cluster nodes (Node Exporter)\n\n- Scrapes Kubernetes metrics (kube-state-metrics)\n\n- Scrapes container metrics (cAdvisor)\n\n- Scrapes Ceph cluster metrics (Ceph Exporter)\n\n- Scrapes custom DebVisor metrics (RPC service, web panel)\n\n### Default Configuration\n\n- URL: [http://prometheus.debvisor-monitoring.svc:9090]([http://prometheus.debvisor-monitoring.svc:909]([http://prometheus.debvisor-monitoring.svc:90]([http://prometheus.debvisor-monitoring.svc:9](http://prometheus.debvisor-monitoring.svc:9)0)9)0)\n\n- Retention: 30 days (configurable)\n\n- Global scrape interval: 30 seconds\n\n- Evaluation interval: 1 minute for alert rules\n\n### Key Queries\n\n## Node CPU usage\n\n    100 *(1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])))\n\n## Pod memory usage\n\n    sum(container_memory_usage_bytes) by (pod_name)\n\n## Ceph cluster usage\n\n    ceph_cluster_used_bytes / ceph_cluster_capacity_bytes\n\n## API latency (p95)\n\n    histogram_quantile(0.95, rate(apiserver_request_duration_seconds_bucket[5m]))\n\n## 2. Grafana\n\n-*Purpose:**Visualization and dashboarding for Prometheus data\n\n### DebVisor Dashboards\n\n| Dashboard | Purpose | Key Metrics |\n|-----------|---------|-------------|\n| `compliance-mfa-audit`| MFA compliance tracking | MFA status, audit events |\n|`debvisor-dns-dashboard`| DNS service health | Query rates, resolution time |\n|`debvisor-multitenant-dashboard`| Multi-tenant isolation | Resource usage per tenant |\n|`dns-dhcp-overview`| DNS/DHCP operations | Zone transfers, DHCP leases |\n|`multi-tenant-isolation`| Tenant network isolation | Traffic patterns, boundaries |\n|`security-overview`| Security events and alerts | Alert firing, remediation status |\n\n### Default Configuration [2]\n\n- URL: [http://grafana.debvisor-monitoring.svc:3000]([http://grafana.debvisor-monitoring.svc:300]([http://grafana.debvisor-monitoring.svc:30]([http://grafana.debvisor-monitoring.svc:3](http://grafana.debvisor-monitoring.svc:3)0)0)0)\n\n- Datasource: Prometheus (UID:`prometheus-debvisor`)\n\n- Default admin: `admin`/`admin`(change in production)\n\n### Dashboard Provisioning\n\n- Dashboards auto-loaded from`provisioning/dashboards/`on startup\n\n- No manual import needed\n\n- Compatible with dashboard-as-code workflows\n\n### 3. Alertmanager\n\n- *Purpose:**Alert routing, grouping, and notification\n\n### DebVisor Alert Receivers\n\n| Receiver | Notification | Use Case |\n|----------|--------------|----------|\n|`email-ops`| Email to ops team | Critical infrastructure alerts |\n|`slack-security`| Slack to security channel | Security events, remediations |\n|`webhook-remediation`| HTTP webhook to Argo Workflows | Trigger automated remediation |\n|`pagerduty`| PagerDuty incident | On-call escalation for critical issues |\n\n### Alert Routing\n\n    default receiver: email-ops\n    +-- route: security-alerts\n    |   +-- receiver: slack-security\n    |       +-- match: alertname ~= "Security.*"\n    +-- route: remediation-alerts\n    |   +-- receiver: webhook-remediation\n    |       +-- match: alertname ~= ".*Remediation.*"\n    +-- route: critical\n        +-- receiver: pagerduty\n            +-- match: severity="critical"\n\n### 4. Prometheus Alerting Rules\n\n- *Location:**`grafana/provisioning/alerting/`\n\n### Rule Categories\n\n| Category | Examples | Severity |\n|----------|----------|----------|\n| Infrastructure | Node down, disk full, memory pressure | Critical/Warning |\n| Storage | Ceph health, ZFS pool degradation | Critical/Warning |\n| Kubernetes | Pod crash loop, deployment not ready | Warning/Info |\n| Network | DNS resolution failures, connectivity issues | Warning/Info |\n| Security | MFA violations, unauthorized access attempts | Critical |\n| Performance | API latency high, request rate high | Warning |\n\n### Alert Propagation\n\n    Metric Value Exceeds Threshold\n        v\n    Prometheus Evaluates Rule\n        v\n    Alert Fires (active for duration threshold)\n        v\n    Alertmanager Receives Alert\n        v\n    Alertmanager Routes to Receiver(s)\n        v\n    Notification Sent (email, Slack, webhook, etc.)\n        v\n    Optional: Auto-Remediation Triggered (via webhook to Argo Workflows)\n\n## Monitoring Setup Paths\n\n### Path 1: DebVisor-Provided Stack (Recommended for Labs)\n\nUse DebVisor's Prometheus + Grafana + Alertmanager:\n\n## Apply DebVisor monitoring stack\n\n    kubectl apply -f opt/monitoring/manifests/prometheus.yaml\n    kubectl apply -f opt/monitoring/manifests/grafana.yaml\n    kubectl apply -f opt/monitoring/manifests/alertmanager.yaml\n\n## Access Grafana\n\n    kubectl port-forward svc/grafana 3000:3000 -n monitoring\n\n## Open [http://localhost:3000,]([http://localhost:3000]([http://localhost:300]([http://localhost:30](http://localhost:30)0)0),) login as admin/admin\n\n## Pros\n\n- Integrated with DebVisor dashboards out-of-box\n\n- Minimal configuration needed\n\n- Pre-configured alert rules\n\n### Cons\n\n- Single-node deployment (OK for labs)\n\n- Limited to Prometheus (no multi-metrics backends)\n\n- Requires Kubernetes cluster\n\n### Path 2: External Prometheus/Grafana\n\nIntegrate with existing monitoring infrastructure:\n\n## Configure Prometheus to scrape DebVisor targets\n\n## Add scrape job\n\n- job_name: 'debvisor-cluster'\n\n        static_configs:\n\n- targets:\n\n- 'debvisor-prometheus:9090'\n\n- 'debvisor-grafana:3000'\n\n## Import DebVisor dashboards into your Grafana\n\n## 1. Download JSON from opt/monitoring/grafana/dashboards/*.json\n\n## 2. Import via Grafana UI: Home -> Import -> Paste JSON\n\n## 3. Select datasource (your existing Prometheus)\n\n## Pros [2]\n\n- Centralized monitoring\n\n- Integrates with existing tools\n\n- Scalable to multiple clusters\n\n### Cons [2]\n\n- Manual setup and configuration\n\n- Must sync dashboards manually\n\n- Requires coordination with existing monitoring\n\n### Path 3: Hybrid (DebVisor + External)\n\nRun DebVisor monitoring, but integrate into central stack:\n\n## Deploy DebVisor monitoring\n\n    kubectl apply -f opt/monitoring/manifests/\n\n## Configure central Prometheus to scrape DebVisor Prometheus\n\n## Central Alertmanager routes to same channels\n\n## Central Grafana queries DebVisor Prometheus\n\n## Synthetic Testing (Labs Only)\n\nFor dashboard and alert testing without real workloads:\n\n## Deploy synthetic metrics generator\n\n    kubectl apply -f opt/monitoring/fixtures/edge-lab-deployment.yaml\n\n## Generate high CPU scenario for 5 minutes\n\n    kubectl set env deployment/synthetic-metrics \\n      ALERT_SCENARIO=high_cpu -n monitoring\n    sleep 300\n    kubectl set env deployment/synthetic-metrics \\n      ALERT_SCENARIO=none -n monitoring\n\n## Cleanup\n\n    kubectl delete -f opt/monitoring/fixtures/edge-lab-deployment.yaml\nSee [FIXTURES_GUIDE.md](FIXTURES_GUIDE.md) for detailed scenarios.\n\n## Metrics Reference\n\n### Standard Prometheus Metrics\n\n### Node Exporter (Host Metrics)\n\n    node_cpu_seconds_total\n    node_memory_MemTotal_bytes\n    node_memory_MemAvailable_bytes\n    node_disk_io_time_seconds_total\n    node_network_receive_bytes_total\n    node_filesystem_avail_bytes\n\n### cAdvisor (Container Metrics)\n\n    container_cpu_usage_seconds_total\n    container_memory_usage_bytes\n    container_network_receive_bytes_total\n    container_last_seen\n\n### Kubernetes (kube-state-metrics)\n\n    kube_node_status_ready\n    kube_pod_status_phase\n    kube_deployment_status_replicas_ready\n    kube_statefulset_status_ready_replicas\n\n### DebVisor-Specific Metrics\n\n### RPC Service\n\n    debvisor_rpc_requests_total\n    debvisor_rpc_request_duration_seconds\n    debvisor_rpc_errors_total\n    debvisor_rpc_auth_failures_total\n\n### Remediation (from Argo Workflows)\n\n    debvisor_remediation_total\n    debvisor_remediation_duration_seconds\n    debvisor_remediation_failed_total\n    debvisor_remediation_alerts_received_total\n\n### Cluster Health\n\n    debvisor_cluster_health_score\n    debvisor_cluster_nodes_ready\n    debvisor_cluster_storage_available_bytes\n\n### Security\n\n    debvisor_security_violations_total\n    debvisor_audit_events_total\n    debvisor_rbac_denials_total\n\n## Best Practices\n\n### Configuration\n\n1.**Retention:**Set appropriate retention for your use case\n\n- Labs: 7 days (minimize disk usage)\n\n- Production: 30-90 days (regulatory requirements)\n\n1.**Scrape Intervals:**Balance between granularity and load\n\n- Default: 30 seconds (good for most cases)\n\n- High-frequency: 10-15 seconds (more granular, higher load)\n\n- Low-frequency: 1-2 minutes (minimum load, less granular)\n\n1.**Alert Thresholds:**Tune based on your environment\n\n- Don't copy thresholds from other clusters\n\n- Start with defaults, adjust based on baselines\n\n- Document why each threshold is set\n\n### Alerting\n\n1.**Alert Severity:**Use consistent severity levels\n\n- Critical: Immediate action required\n\n- Warning: Action needed within hours\n\n- Info: Informational, no immediate action\n\n1.**Alert Routing:**Send to appropriate receivers\n\n- Critical -> On-call PagerDuty\n\n- Security -> Security team Slack\n\n- Infrastructure -> Ops team email\n\n1.**Avoid Alert Fatigue:**\n\n- Adjust thresholds to reduce false positives\n\n- Group related alerts\n\n- Use silence rules for known maintenance windows\n\n### Dashboard Design\n\n1.**Clarity:**Each panel should be self-explanatory\n\n- Use descriptive titles\n\n- Include units (e.g., "%", "MB/s")\n\n- Document non-obvious panels in comments\n\n1.**Hierarchy:**Organize panels logically\n\n- Top: Overall cluster health\n\n- Middle: Component status\n\n- Bottom: Detailed metrics\n\n1.**Updates:**Keep dashboards current\n\n- Review quarterly for stale or missing metrics\n\n- Update when new services added\n\n- Archive outdated dashboards\n\n## Troubleshooting\n\n### No Data in Grafana\n\n1.**Check Prometheus targets:**[http://prometheus:9090/targets]([http://prometheus:9090/target]([http://prometheus:9090/targe]([http://prometheus:9090/targ](http://prometheus:9090/targ)e)t)s)\n1.**Check scrape errors:**[http://prometheus:9090/graph]([http://prometheus:9090/grap]([http://prometheus:9090/gra]([http://prometheus:9090/gr](http://prometheus:9090/gr)a)p)h) -> query`up`\n1.**Verify datasource:**Grafana -> Configuration -> Datasources\n1.**Check dashboard JSON:**Ensure metric names are correct\n\n### High Prometheus Memory Usage\n\n1.**Check cardinality:**High-cardinality metrics use more memory\n\n- Use`promtool analyze cardinality`to identify culprits\n\n- Consider dropping labels or rewriting queries\n\n1.**Reduce retention:**Lower`--storage.tsdb.retention.time`\n1.**Disable unused scrape jobs:**Comment out unused targets\n\n### Alert Not Firing\n\n1.**Check rule evaluation:**[http://prometheus:9090/rules]([http://prometheus:9090/rule]([http://prometheus:9090/rul]([http://prometheus:9090/ru](http://prometheus:9090/ru)l)e)s)\n1.**Verify metric exists:**[http://prometheus:9090/graph]([http://prometheus:9090/grap]([http://prometheus:9090/gra]([http://prometheus:9090/gr](http://prometheus:9090/gr)a)p)h) -> query metric name\n1.**Check threshold:**Ensure current value exceeds alert threshold\n1.**Check duration:**Alert must exceed `for:`duration before firing\n\n### Alertmanager Not Sending Notifications\n\n1.**Check Alertmanager status:**[http://alertmanager:9093]([http://alertmanager:909]([http://alertmanager:90]([http://alertmanager:9](http://alertmanager:9)0)9)3)\n1.**Verify receiver config:**Check`alertmanager.yml`syntax\n1.**Test webhook:**Use`curl`to post test alert to webhook receiver\n1.**Check logs:**`kubectl logs deployment/alertmanager -n monitoring`\n\n## Related Documentation\n\n- [Grafana README](grafana/README.md) - Dashboard setup and datasources\n\n- [Fixtures Guide](FIXTURES_GUIDE.md) - Synthetic metrics for testing\n\n- [Prometheus Docs]([https://prometheus.io/docs]([https://prometheus.io/doc]([https://prometheus.io/do](https://prometheus.io/do)c)s)/) - Official Prometheus documentation\n\n- [Grafana Docs]([https://grafana.com/docs/grafana/latest]([https://grafana.com/docs/grafana/lates]([https://grafana.com/docs/grafana/late](https://grafana.com/docs/grafana/late)s)t)/) - Official Grafana documentation\n\n- [Alerting Best Practices]([https://prometheus.io/docs/alerting/latest/best_practices]([https://prometheus.io/docs/alerting/latest/best_practice]([https://prometheus.io/docs/alerting/latest/best_practic](https://prometheus.io/docs/alerting/latest/best_practic)e)s)/) - Alert design guidance\n\n## Support\n\n- **Issue:**File in main DebVisor repository\n\n- **Documentation:**Check this README and linked guides\n\n- **Dashboards:**See Grafana folder structure under`dashboards/`\n\n- **Fixtures:**See `FIXTURES_GUIDE.md` for synthetic metrics testing\n\n- --\n\n- *Last Updated:**2025-11-26\n\n- *Status:**Monitoring infrastructure documented and ready for deployment\n\n### Next Steps\n\n1. Deploy monitoring stack to your cluster\n\n1. Verify all targets are scraping successfully\n\n1. Test Grafana dashboards with real or synthetic data\n\n1. Configure alert notification channels\n\n1. Test alert firing and remediation workflows\n\n
