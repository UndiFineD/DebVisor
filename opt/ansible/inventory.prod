# DebVisor Ansible Inventory - PRODUCTION ENVIRONMENT
#
# This template provides a production-grade inventory with HA, security, and monitoring.
# BEFORE USING IN PRODUCTION:
#   1. Replace all example IPs with actual production IPs
#   2. Update domain names (example.com â†’ your-domain.com)
#   3. Review and adjust all security settings
#   4. Use ansible-vault for sensitive variables
#   5. Test thoroughly in staging environment first
#   6. Enable host_key_checking = True in ansible.cfg
#   7. Use SSH key pairs with proper access controls
#
# To use:
#   ansible-playbook -i inventory.prod --ask-vault-pass playbooks/site.yml
#
# SECURITY NOTES:
#   - All IPs shown as examples; update to real production IPs
#   - Use private networks for management and cluster traffic
#   - Enable TLS for all inter-service communication
#   - Use ansible-vault for credentials and secrets

---
all:
  vars:
    # ========== GLOBAL PRODUCTION SETTINGS ==========
    domain_name: "example.com"
    environment: "production"
    ntp_servers:
      - time1.example.com
      - time2.example.com
      - time.cloudflare.com
    dns_servers:
      - 8.8.8.8
      - 8.8.4.4

    # ========== SECURITY SETTINGS ==========
    ssh_port: 22
    enable_selinux: true
    firewall_enabled: true
    require_mfa: true
    tls_enabled: true
    audit_enabled: true

    # ========== MONITORING & LOGGING ==========
    syslog_server: "logs.example.com"
    syslog_port: 514
    prometheus_retention: "30d"
    elasticsearch_url: "https://elk.example.com:9200"

    # ========== BACKUP & DISASTER RECOVERY ==========
    backup_enabled: true
    backup_destination: "s3://backups-example/debvisor/"
    backup_retention_days: 30
    backup_schedule: "0 2 * * *"  # 2 AM daily

  children:
    all_dns:
      children:
        dns_primaries:
          vars:
            bind_role: "primary"
            tsig_key_rotation: "weekly"
            dns_notify_secondaries: true
            dnssec_enabled: true
            enable_audit_log: true
          hosts:
            dns01.prod.example.com:
              ansible_host: 10.100.0.10
              bind_listen_ipv4:
                - 10.100.0.10
                - 127.0.0.1
              dns_zones:
                - "example.com"
                - "prod.example.com"
                - "0.0.100.in-addr.arpa"
              tsig_key_file: "/etc/bind/keys/tsig-ha.key"
              monitoring_port: 8765
            dns02.prod.example.com:
              ansible_host: 10.100.0.11
              bind_listen_ipv4:
                - 10.100.0.11
                - 127.0.0.1
              dns_zones:
                - "example.com"
                - "prod.example.com"
              monitoring_port: 8765

        dns_secondaries:
          vars:
            bind_role: "secondary"
            allow_transfer_from:
              - 10.100.0.10
              - 10.100.0.11
            zone_transfer_retry: 3600
            enable_audit_log: true
          hosts:
            dns03.prod.example.com:
              ansible_host: 10.100.0.12
              bind_listen_ipv4:
                - 10.100.0.12
                - 127.0.0.1
              primary_nameserver: "dns01.prod.example.com"
              monitoring_port: 8765
            dns04.prod.example.com:
              ansible_host: 10.100.0.13
              bind_listen_ipv4:
                - 10.100.0.13
                - 127.0.0.1
              primary_nameserver: "dns02.prod.example.com"
              monitoring_port: 8765

    all_ceph:
      vars:
        ceph_version: "reef"
        ceph_cluster_name: "ceph-prod"
        ceph_osd_pool_default_size: 3
        ceph_osd_pool_default_min_size: 2
        ceph_mon_clock_drift_allowed: 0.05
        ceph_health_check_interval: 60
        ceph_recovery_max_active: 5
        ceph_recovery_max_single_active: 1
        ceph_recovery_sleep: 0.001
        enable_ceph_monitoring: true
        ceph_dashboard_enabled: true

      children:
        ceph_mons:
          vars:
            mon_role: "monitor"
            mon_max_pg_per_osd: 300
            mon_pg_warn_max_object_skew: 10
            enable_audit_log: true
          hosts:
            ceph-mon01.prod.example.com:
              ansible_host: 10.100.1.10
              ceph_address: 10.100.1.10
              mon_initial_members:
                - ceph-mon01
                - ceph-mon02
                - ceph-mon03
              monitor_interface: "bond0"
              cluster_network: "10.100.1.0/24"
            ceph-mon02.prod.example.com:
              ansible_host: 10.100.1.11
              ceph_address: 10.100.1.11
              monitor_interface: "bond0"
              cluster_network: "10.100.1.0/24"
            ceph-mon03.prod.example.com:
              ansible_host: 10.100.1.12
              ceph_address: 10.100.1.12
              monitor_interface: "bond0"
              cluster_network: "10.100.1.0/24"

        ceph_osds:
          vars:
            osd_role: "osd"
            osd_memory_target: 8589934592  # 8GB for production
            osd_recovery_sleep: 0.001
            enable_crush_device_classes: true
            osd_max_backfills: 10
            osd_recovery_max_active: 8
          hosts:
            ceph-osd01.prod.example.com:
              ansible_host: 10.100.1.20
              ceph_address: 10.100.1.20
              osd_devices:
                - /dev/sdb
                - /dev/sdc
                - /dev/sdd
              cluster_network: "10.100.1.0/24"
            ceph-osd02.prod.example.com:
              ansible_host: 10.100.1.21
              ceph_address: 10.100.1.21
              osd_devices:
                - /dev/sdb
                - /dev/sdc
                - /dev/sdd
              cluster_network: "10.100.1.0/24"
            ceph-osd03.prod.example.com:
              ansible_host: 10.100.1.22
              ceph_address: 10.100.1.22
              osd_devices:
                - /dev/sdb
                - /dev/sdc
                - /dev/sdd
              cluster_network: "10.100.1.0/24"
            ceph-osd04.prod.example.com:
              ansible_host: 10.100.1.23
              ceph_address: 10.100.1.23
              osd_devices:
                - /dev/sdb
                - /dev/sdc
                - /dev/sdd
              cluster_network: "10.100.1.0/24"

    all_kubernetes:
      vars:
        kubernetes_version: "1.28.0"
        kubernetes_cni: "calico"
        kubernetes_pod_network_cidr: "10.244.0.0/16"
        kubernetes_service_cidr: "10.96.0.0/12"
        kubelet_max_pods: 110
        enable_rbac: true
        enable_network_policy: true
        enable_psp: true
        enable_audit: true

      children:
        k8s_controlplane:
          vars:
            kube_role: "control-plane"
            etcd_cluster_size: 3
            kube_api_port: 6443
            kube_controller_manager_port: 10252
            enable_audit_logging: true
          hosts:
            k8s-ctrl01.prod.example.com:
              ansible_host: 10.100.2.10
              kube_apiserver_advertise_address: 10.100.2.10
              etcd_name: "etcd01"
              backup_schedule: "0 1 * * *"
            k8s-ctrl02.prod.example.com:
              ansible_host: 10.100.2.11
              kube_apiserver_advertise_address: 10.100.2.11
              etcd_name: "etcd02"
              backup_schedule: "0 1 * * *"
            k8s-ctrl03.prod.example.com:
              ansible_host: 10.100.2.12
              kube_apiserver_advertise_address: 10.100.2.12
              etcd_name: "etcd03"
              backup_schedule: "0 1 * * *"

        k8s_workers:
          vars:
            kube_role: "worker"
            kubelet_extra_args: "--max-pods=110"
            container_runtime: "containerd"
            enable_workload_monitoring: true
          hosts:
            k8s-worker01.prod.example.com:
              ansible_host: 10.100.2.20
              node_labels: "workload=general,zone=a"
              node_taints: []
            k8s-worker02.prod.example.com:
              ansible_host: 10.100.2.21
              node_labels: "workload=general,zone=b"
              node_taints: []
            k8s-worker03.prod.example.com:
              ansible_host: 10.100.2.22
              node_labels: "workload=general,zone=c"
              node_taints: []
            k8s-worker04.prod.example.com:
              ansible_host: 10.100.2.23
              node_labels: "workload=storage,zone=a"
              node_taints:
                - "node-type=storage:NoSchedule"
            k8s-worker05.prod.example.com:
              ansible_host: 10.100.2.24
              node_labels: "workload=storage,zone=b"
              node_taints:
                - "node-type=storage:NoSchedule"

    hypervisors:
      vars:
        libvirt_uri: "qemu+tls://hypervisor.example.com/system"
        vm_default_cpu_mode: "host-passthrough"
        vm_disk_pool: "/var/lib/libvirt/images"
        enable_live_migration: true
        enable_vm_monitoring: true
      hosts:
        hv01.prod.example.com:
          ansible_host: 10.100.3.10
          hypervisor_name: "hv01"
          kvm_capabilities: "nested"
        hv02.prod.example.com:
          ansible_host: 10.100.3.11
          hypervisor_name: "hv02"
          kvm_capabilities: "nested"
        hv03.prod.example.com:
          ansible_host: 10.100.3.12
          hypervisor_name: "hv03"
          kvm_capabilities: "nested"

    management:
      vars:
        is_management_node: true
        prometheus_port: 9090
        grafana_port: 3000
        enable_ha: true
        backup_enabled: true
      hosts:
        mgmt01.prod.example.com:
          ansible_host: 10.100.4.10
          grafana_admin_password: "{{ vault_grafana_admin_password }}"
          prometheus_scrape_interval: "15s"
          backup_schedule: "0 0 * * *"
        mgmt02.prod.example.com:
          ansible_host: 10.100.4.11
          grafana_admin_password: "{{ vault_grafana_admin_password }}"
          prometheus_scrape_interval: "15s"
          backup_schedule: "0 0 * * *"
