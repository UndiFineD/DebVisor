# Live Migration & Failover\n\n## Overview\n\nDebVisor enables low-downtime relocation of running VMs across nodes using libvirt/QEMU live migration and shared Ceph RBD storage.\n\n## Requirements\n\n- Shared RBD pools accessible from source & target nodes.\n\n- Matching CPU feature sets (ideally identical hardware or `host-model`).\n\n- Stable network for migration channel (TLS protected).\n\n- Sufficient bandwidth; consider enabling compression.\n\n## Modes\n\n| Mode | Notes |\n|------|-------|\n| Pre-Copy | Iteratively copies dirtied pages; short final pause. Default. |\n| Post-Copy | Starts VM on target then fetches remaining pages; risk if network unstable. |\n\n## Command Examples\n\n## Pre-copy live migration\n\n    domain="vm1"\n    virsh migrate --live --persistent --compressed --p2p --tunnelled qemu:///system qemu+tls://target/system $domain\n\n## Post-copy trigger after pre-copy start\n\n    virsh domjobinfo $domain\n    virsh migrate-postcopy $domain\n\n## Performance Tuning\n\n- Enable compression: `--compressed --comp-methods zlib`.\n\n- Use dedicated migration network (separate NIC/VLAN) for large VMs.\n\n- Adjust dirty page rate with guest ballooning during migration if needed.\n\n## Failover Strategy\n\n1. Health monitor detects node failure (missed heartbeats).\n\n1. Mark affected VMs for recovery (policy?protected list).\n\n1. Verify source host fencing (prevent double start).\n\n1. Start VMs on target using existing RBD volumes.\n\n## Fencing Considerations\n\n- Soft failure: attempt graceful migration first.\n\n- Hard failure: ensure power fencing (IPMI/Redfish) before restart to avoid split?brain.\n\n## Post-Migration Hooks\n\n- Update VM location mapping table.\n\n- Emit audit record (old_node, new_node, duration, result).\n\n- Trigger optional re-registration in DNS if IP changes (rare if bridged static).\n\n## Metrics\n\n- Migration duration (total + downtime).\n\n- Dirty page iterations count.\n\n- Bandwidth utilized.\n\n- Success/failure rate; reasons categorized (network, CPU mismatch, storage latency).\n\n## Roadmap Enhancements\n\n- Automatic selection of optimal target (least loaded, fastest network path).\n\n- Predictive pre-warming using historical memory change rates.\n\n- Integration with scheduling to defragment resource usage.\n\n## Interoperability with Other Hypervisors\n\nDebVisor is designed to coexist with, and migrate workloads to/from,\nother virtualization platforms rather than replace them outright.\n\n### Importing VMs into DebVisor\n\n- **Disk format conversion**:\n\n- Use `scripts/debvisor-vm-convert.sh` to convert guest disks from\n\n      formats such as `vmdk`or`raw` into DebVisor's preferred\n      `qcow2` (or vice versa).\n\n- Example:\n\n    debvisor-vm-convert.sh \\n\n      - -from vmdk \\n\n      - -to qcow2 \\n\n      - -in  /path/from-vsphere/appliance.vmdk \\n\n      - -out /var/lib/libvirt/images/appliance.qcow2\n\n- **Cloud-style images and cloud-init**:\n\n- For images that expect cloud-init (Ubuntu Cloud, GenericCloud,\n\n        etc.), build a seed ISO with\n        `scripts/debvisor-cloudinit-iso.sh` and attach it alongside the\n        converted disk.\n\n- This lets the guest configure hostname, SSH keys and basic\n\n        settings exactly as it would in a cloud environment.\n\n- **Networking alignment**:\n\n- Map the imported VM's NICs to DebVisor bridges/VLANs that provide\n\n        equivalent connectivity (for example mapping a "DMZ" network to a\n        DebVisor VLAN-backed bridge).\n\n- Where possible, keep IP addressing stable by reusing the same\n\n        subnets and DHCP reservations or static mappings.\n\n### Exporting VMs from DebVisor\n\n- To move a VM out of DebVisor:\n\n- Shut down the guest cleanly.\n\n- Use `debvisor-vm-convert.sh` to produce a disk image in the\n\n        target platform's preferred format (for example `vmdk` for\n        vSphere, `raw` for some clouds or bare-metal orchestrators).\n\n- Import or register that disk with the target hypervisor using its\n\n        native tools, then recreate the VM definition (CPU, RAM, NIC\n        mapping) to match.\n\n- For cloud-init-based guests, you can often reuse the same\n\n      `user-data`and`meta-data` content you used on DebVisor, adjusting\n      only platform-specific fields.\n\n#### Example: libvirt XML for imported cloud image\n\n  vm1\n  4096\n  2\n    hvm\n\n- The first disk points to the imported or converted cloud image\n\n  (for example from `debvisor-vm-convert.sh`).\n\n- The CD-ROM attaches the `cidata` ISO created by\n\n  `debvisor-cloudinit-iso.sh`, which cloud-init consumes at boot.\n\n- Networking is bridged via `br0`; adjust bridge name, resources, and\n\n  machine type to match your environment.\n\n### Mixed Environments\n\n- DebVisor can participate in a broader environment where some\n\n  workloads run on external clouds or virtualization platforms while\n  others live on DebVisor nodes.\n\n- Recommended patterns:\n\n- Use a shared configuration source (for example Git + Ansible or\n\n    GitOps) to keep VM and container definitions consistent across\n    platforms.\n\n- Expose metrics from DebVisor Prometheus and from external\n\n    platforms into a common Grafana instance for a unified view.\n\n- Use DNS as the primary abstraction: service names remain stable\n\n    while the backing VM or container can move between DebVisor and\n    other environments.\n\n
