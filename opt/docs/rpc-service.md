# DebVisor RPC Service Design\n\n## Purpose\n\nCoordinate node membership, health, migration, and configuration synchronization securely.\n\n## Deployment\n\n- **Service User**: `debvisor-rpc:debvisor-rpc` (system account, no interactive login)\n\n- **Working Directory**: `/opt/debvisor/rpc`\n\n- **Port**: TCP `9443`(defined as`rpc_port`in nftables; inbound disabled by default, enable via`debvisor.nft`or`debvisor-local.nft` as needed)\n\n- **Systemd Unit**: `debvisor-rpcd.service`\n\n- Depends on `network-online.target`\n\n- Logs with `SyslogIdentifier=debvisor-rpcd`\n\n- Restarts on failure\n\n- **First-boot setup**: `debvisor-firstboot.sh`calls`debvisor-setup-rpc-user.sh`to create the service account and set ownership of`/opt/debvisor/rpc`\n\n## Stack\n\n- gRPC (mutual TLS)\n\n- etcd (registry + coordination keys)\n\n- Language: Python (gRPC)\n\n## Certificates\n\n- Issued by internal CA (cfssl or Keycloak). Stored under `/etc/debvisor/pki/`.\n\n- Rotation: systemd timer calls renewal + distribution via secure RPC.\n\n## Coordination & State (etcd)\n\n`debvisor-rpc` uses etcd as the runtime coordination store. Desired state (policies, profiles) is managed via Git and rendered into versioned config blobs, while etcd holds fast-moving operational state:\nAt a high level, the node agent (implemented as part of `debvisor-rpcd` or a small companion) is responsible for:\n\n- Writing `/debvisor/self/node_id` on each node to a stable identifier\n\n- Maintaining `/debvisor/nodes//info`(including`fqdn`) and`/debvisor/nodes//mode`\n\n- Ensuring these keys are updated whenever local configuration (hostname, IP, mode) changes\n\nThe hostname registration script (`/usr/local/bin/hostname-register.sh`) is a consumer of this data: it prefers the cluster-assigned FQDN from`/debvisor/nodes//info/fqdn` when present, but falls back to the local hostname if etcd or the key is unavailable.\n\n### Nodes & Health\n\n    /debvisor/nodes//info          # static info: hostname, IP, rack, version\n    /debvisor/nodes//health        # latest Health snapshot from Heartbeat\n    /debvisor/nodes//last_seen     # RFC3339 timestamp of last heartbeat\n    /debvisor/nodes//status        # OK | DEGRADED | DOWN (derived by controller)\n     /debvisor/nodes//mode          # standalone | clustered (influences hostname/DNS behavior)\n     /debvisor/nodes//info/fqdn     # optional cluster-assigned FQDN used by hostname-register\n\n### VM Placement & Locks\n\n    /debvisor/vms//owner           # current authoritative node for the VM\n    /debvisor/vms//lock            # ephemeral lock holder (node_id) with TTL lease\n    /debvisor/vms//metadata        # JSON/YAML with policy flags (protected, HA group, etc.)\n\n### Replication Jobs & Last Sync\n\n    /debvisor/replication/jobs/         # job definition (source, target, backend, dataset, policy)\n    /debvisor/replication/jobs//status # job state machine + last run metrics\n    /debvisor/replication/zfs///last_snapshot   # last successfully replicated snapshot name\n    /debvisor/replication/zfs///last_success_at # timestamp of last successful run\n\n### Templates (Ceph RBD)\n\n    /debvisor/templates/              # template metadata: backend=rbd, pool, image, default snapshot label\n\n### Config Sync\n\n    /debvisor/config/current_version              # desired config version (e.g. Git SHA or semantic version)\n    /debvisor/config/blob                         # rendered desired-state blob or pointer to CephFS path\n    /debvisor/nodes//config/version      # last applied config version on the node\n    /debvisor/nodes//config/last_applied_at # timestamp of last successful apply\n    /debvisor/nodes//config/status       # OK | STALE | ERROR (+ optional error details)\n\n## Health Metrics\n\n- libvirt reachable\n\n- Ceph `ceph -s` status parsed -> HEALTH_OK / WARN / ERR\n\n- CPU/RAM pressure thresholds\n\n- Network latency (gRPC round-trip to peers)\n\n## RPC Endpoints\n\n| Method | Description |\n|--------|-------------|\n| RegisterNode(NodeInfo) | Add/update node metadata |\n| Heartbeat(Health) | Push health snapshot |\n| ListNodes(Empty) | Enumerate cluster nodes |\n| PrepareMigration(VMRef, Target) | Validate migration readiness |\n| MigrateVM(VMMigrateSpec) | Execute live migration (libvirt) |\n| FailoverVM(VMRef) | Promote or restart VM on alternate node |\n| SnapshotRBD(VMRef, Label) | Create snapshot for VM disk |\n| RollbackRBD(VMRef, Label) | Restore snapshot |\n| SyncDirectory(Empty) | Refresh LDAP/AD cache |\n| DispatchConfig(ConfigBlob) | Push config to nodes (versioned) |\n\n## Migration Flow\n\n1. `PrepareMigration` checks CPU flags, shared RBD access, network.\n\n1. `MigrateVM`issues`virsh migrate --live --p2p --tunnelled` command.\n\n1. Monitors progress (libvirt events); updates etcd status.\n\n1. Finalizes by updating active node reference.\n\n## Failover Flow\n\n1. Heartbeats fail for source node.\n\n1. Determine protected VMs (policy list).\n\n1. Attempt `virsh list --all` via last known connection (optional fence).\n\n1. Start VM on healthy node referencing same RBD volume.\n\n## Security\n\n- mTLS for all RPC calls.\n\n- Role checks: only `debvisor-operator`and`debvisor-admin` groups can invoke migration/failover endpoints.\n\n- Audit log appended with request metadata (user, timestamp, VM UUID, action).\n\n## Roadmap\n\n- Phase 1: Node registry, heartbeat, migratevm (Implemented).\n\n- Phase 2: Failover orchestration & snapshots (Implemented).\n\n- Phase 3: Config distribution & certificate rotation automation (Planned).\n\n- Phase 4: Multi-cluster federation (stretch / geo replication) (Planned).\n
